## MapPartitions提升Map类的操作数
1. spark中, 最基本的原则, 就是每个task处理一个RDD的partition
2. 普通的map, 一条一条的处理, 比如一个partition中有1万条数据, 那么你的function就需要执行和计算1万次
3. 但是, 使用MapPartition操作后, 一个task仅执行一次function, function一次接受所有的partition数据

## filter过后使用coalesce算子减少分区
1. 过滤后可能会导致数据倾斜
2. 优化: 压缩partition

## 使用repartition算子解决SparkSQL并行度低的问题
1. repartition对RDD进行重分区