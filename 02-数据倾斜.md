# 数据倾斜

## 现象
大部分的task执行很快, 但是个别task执行很慢. 比如共1000个task, 其中的997个task都在1分钟以内执行完了, 剩余的3个task却要1-2个小时

## 原理
在执行shuffle时, 必须将各个节点上相同的key拉取到某个节点上的一个task来处理, 比如按照key进行聚合或join操作. 如果某个key对应的数据量很大, 就会发生数据倾斜. 比如大部分key对应10条数据, 但是个别key却对应了100万条数据, 那么大部分task可能只会分配到10条数据, 1s就执行完成了; 但是个别task可能分配了100万数据, 要运行1-2个小时, 可能还是内存溢出

## 定位原因
1. 去sparkUI上查看各个job的耗时, job下各个stage的耗时, 找到耗时较长的stage以及task, 此task的数据量会远超过其他的task
2. 如果是sparkSQL中的group by, join导致的数据倾斜, 查询key的分布情况


# 解决数据倾斜
## 过滤掉少量导致数据倾斜的key
1. 适用场景: 少量几个key导致数据倾斜, 并且这几个key对结果没有影响
2. 实现方案: 用where过滤掉这个key, 使其不参与计算
3. 优点: 实现简单, 效果好, 可以完全规避数据倾斜
4. 缺点: 适用场景不多, 大多数情况下, 导致倾斜的key还是很多的

## 提高shuffle的并行度
1. 适用场景: 
2. 实现方案: spark.sql.shuffle.partitions=200(默认是200), shuffle read task的并行度
3. 实现原理: 增加shuffle read task的数量, 可以让原本分配给一个task的多个key分配给多个task, 从而让每个task处理比原来更少的数据. 举例来说, 原本有5个key, 每个key对应10条数据, 这5个key都是分配给1个task的, 那么这个task就要处理50条数据. 而增加shuffle read task后, 每个task就分配到一个key, 即每个task就处理10条数据, 那么自然每个task执行时间变短
4. 优点: 简单, 可以有效缓解和减轻数据倾斜的影响
5. 缺点: 只是缓解了数据倾斜, 没有根除, 效果有限

## 两阶段聚合
1. 适用场景: 在Spark SQL中使用group by语句进行聚合时, 比较适用
2. 实现思路: 分为两阶段聚合. 第一次, 在每个key上加上一个随机数(比如10以内的随机数), 接着对加了随机数的数据, 执行group by操作. 第二次, 将各个key的随机数去掉, 再做一次聚合
3. 优点: 对聚合类的shuffle操作导致的数据倾斜, 效果很不错
4. 缺点: 仅适用于聚合类的shuffle操作导致的数据倾斜. 如果是join类的shuffle操作, 还要改用其他方案

## 将reduce join改为map join
1. 适用场景: 在Spark SQL中使用join时, 并且join操作的一个RDD或表的数据量比较小, 比较适用该方案
2. 实现思路: 不使用join算子进行连接操作, 而使用broadcast变量与map类算子实现join操作, 进而完全规避掉shuffle类的操作, 彻底避免数据倾斜的产生和出现. 将较小的RDD中的数据直接通过collect算子拉取到Driver端的内存中, 然后对其创建一个Broadcast变量; 接着对另一个RDD执行map类算子, 在算子函数内, 从Broadcast变量中获取较小RDD的全量数据, 与当前RDD的每条数据按照连接key进行比对, 如果连接key相等, 那么将两个RDD的数据连接起来
3. 实现原理: 普通的join是会走shuffle过程的, 而一旦shuffle, 就相当于将相同key的数据拉取到一个shuffle read task中再进行join, 此时就是reduce join. 但如果一个RDD比较小时, 可以采用广播小RDD的全量数据+map算子来实现与join相同的效果, 也就是map join, 此时就不会发生shuffle操作, 也就不会发生数据倾斜
4. 优点: 对join操作导致的数据倾斜, 效果非常好, 因为根本就不会发生shuffle, 也就根本不会发生数据倾斜
5. 缺点: 适用场景较少, 只适用于一个大表和一个小表的情况. 毕竟将小表广播, 比较消耗内存资源, driver和每个executor内存中都会存放一份小RDD的全量数据. 如果广播出去的RDD比较大, 比如10G以上, 那么就可能发生内存溢出, 因此不适用于两个都是大表的情况

## 采样倾斜key并分拆join操作
1. 适用场景: 两个RDD/Hive表进行join时, 如果数据量都比较大, 无法做广播操作, 那么此时可以看一下两个RDD/Hive表的key分布情况. 如果出现数据倾斜, 是因为其中一个RDD/Hive表中的少量几个key的数据量过大, 而另一个RDD/Hive表中的所有key都分布比较均匀, 那么采用该方案是比较合适的
2. 实现思路
- 统计每个key的数量, 计算出数据量较大的几个key
- 将这几个key对应的数据从原来的RDD拆分出来, 形成一个单独的RDD, 并给每个key打上`n`以内随机数, 而不会导致数据倾斜的大部分key形成另一个RDD
- 接着, 将需要join的另一个`RDD`, 也过滤出来那几个倾斜key对应的数据, 并形成一个单独的`RDD`, 将每条数据复制成`n`份, 这n条数据都按顺序附加一个0~n的随机数, 不会导致倾斜的大部分key也形成另一个RDD
- 再将附加了随机数的RDD与另一个复制了n份的独立RDD进行join, 此时就可以将原先相同的key打散成n份, 分散到多个task中去join
- 另外两个正常的RDD就直接join
- 最后, 将两次join的结果适用union算子合并起来
3. 实现原理: 对于join导致的数据倾斜, 如果只是某几个key导致了倾斜, 可以将少量几个key分拆成独立的RDD, 并添加随机数打散成n份去join, 此时这几个key对应的数据就不会集中在少量几个task上, 而是分散到多个task上去join
4. 优点: 只需要对少量倾斜的key复制n份, 不需要对全量数据进行扩容, 避免了占用过多内存
5. 缺点: 如果导致倾斜的key特别多时, 该方案不合适

## 使用随机数和扩容RDD进行join
1. 实现思路: 对另一个正常的RDD进行扩容, 每条数据扩容成n份